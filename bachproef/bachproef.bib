% Encoding: UTF-8

@misc{akkiraju2024factsbuildingretrievalaugmented,
  author        = {Rama Akkiraju and Anbang Xu and Deepak Bora and Tan Yu and Lu An and Vishal Seth and Aaditya Shukla and Pritam Gundecha and Hridhay Mehta and Ashwin Jha and Prithvi Raj and Abhinav Balasubramanian and Murali Maram and Guru Muthusamy and Shivakesh Reddy Annepally and Sidney Knowles and Min Du and Nick Burnett and Sean Javiya and Ashok Marannan and Mamta Kumari and Surbhi Jha and Ethan Dereszenski and Anupam Chakraborty and Subhash Ranjan and Amina Terfai and Anoop Surya and Tracey Mercer and Vinodh Kumar Thanigachalam and Tamar Bar and Sanjana Krishnan and Samy Kilaru and Jasmine Jaksic and Nave Algarici and Jacob Liberman and Joey Conway and Sonu Nayyar and Justin Boitano},
  title         = {FACTS About Building Retrieval Augmented Generation-based Chatbots},
  eprint        = {2407.07858},
  url           = {https://arxiv.org/abs/2407.07858},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  year          = {2024}
}

@misc{chiang2024chatbotarenaopenplatform,
  title         = {Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
  author        = {Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},
  year          = {2024},
  eprint        = {2403.04132},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2403.04132}
}


@misc{ganesan2018rouge20updatedimproved,
  title         = {ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks},
  author        = {Kavita Ganesan},
  year          = {2018},
  eprint        = {1803.01937},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  url           = {https://arxiv.org/abs/1803.01937}
}

@misc{gao2024retrievalaugmentedgenerationlargelanguage,
  title         = {Retrieval-Augmented Generation for Large Language Models: A Survey},
  author        = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
  year          = {2024},
  eprint        = {2312.10997},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2312.10997}
}

@inbook{Gupta2025,
  author    = {Gupta, Rajan and Tiwari, Sanju and Chaudhary, Poonam},
  booktitle = {Generative AI: Techniques, Models and Applications},
  title     = {Large Language Models},
  doi       = {10.1007/978-3-031-82062-5_5},
  isbn      = {978-3-031-82062-5},
  pages     = {81-102},
  publisher = {Springer Nature Switzerland},
  url       = {https://doi.org/10.1007/978-3-031-82062-5_5},
  abstract  = {Large Language Models have presented an impressive performance and have become fundamental in real-world applications. These models are built upon vast amounts of text data and are trained to understand, generate, and manipulate human language with remarkable fluency. Large language models have applications in various fields, including content generation, language translation, sentiment analysis, and conversational agents. These models are based on neural networks and considered as pre-trained, large-scale, statistical language models. LLMs are playing a significant role in advancement of AI agents. This chapter has explored different existing surveys and summarized in two categories, 7 general survey papers and 15 domain specific survey papers. Primary focus of chapter is to explore the types of large language models, tasks of LLMs, frame-works, applications and challenges.},
  address   = {Cham},
  year      = {2025}
}

@Online{microsoft2024evaluation,
  author  = {{Microsoft Corporation}},
  title   = {A list of metrics for evaluating LLM-generated content},
  url     = {https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics#reference-based-metrics},
  urldate = {2025-04-21},
  year    = {2024},
}

@inproceedings{papineni-etal-2002-bleu,
  author    = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  title     = {BLEU: a method for automatic evaluation of machine translation},
  year      = {2002},
  publisher = {Association for Computational Linguistics},
  address   = {USA},
  url       = {https://doi.org/10.3115/1073083.1073135},
  doi       = {10.3115/1073083.1073135},
  abstract  = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
  booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
  pages     = {311-318},
  numpages  = {8},
  location  = {Philadelphia, Pennsylvania},
  series    = {ACL '02}
}

@misc{vaswani2023attentionneed,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1706.03762}
}

@misc{wang2024searchingbestpracticesretrievalaugmented,
  title         = {Searching for Best Practices in Retrieval-Augmented Generation},
  author        = {Xiaohua Wang and Zhenghua Wang and Xuan Gao and Feiran Zhang and Yixin Wu and Zhibo Xu and Tianyuan Shi and Zhengyuan Wang and Shizheng Li and Qi Qian and Ruicheng Yin and Changze Lv and Xiaoqing Zheng and Xuanjing Huang},
  year          = {2024},
  eprint        = {2407.01219},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2407.01219}
}

@misc{wu2025retrievalaugmentedgenerationnaturallanguage,
  title         = {Retrieval-Augmented Generation for Natural Language Processing: A Survey},
  author        = {Shangyu Wu and Ying Xiong and Yufei Cui and Haolun Wu and Can Chen and Ye Yuan and Lianming Huang and Xue Liu and Tei-Wei Kuo and Nan Guan and Chun Jason Xue},
  year          = {2025},
  eprint        = {2407.13193},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2407.13193}
}

@misc{xu2024retrievalmeetslongcontext,
  title         = {Retrieval meets Long Context Large Language Models},
  author        = {Peng Xu and Wei Ping and Xianchao Wu and Lawrence McAfee and Chen Zhu and Zihan Liu and Sandeep Subramanian and Evelina Bakhturina and Mohammad Shoeybi and Bryan Catanzaro},
  year          = {2024},
  eprint        = {2310.03025},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2310.03025}
}

@inbook{Yu_2025,
  title     = {Evaluation of Retrieval-Augmented Generation: A Survey},
  isbn      = {9789819610242},
  issn      = {1865-0937},
  url       = {http://dx.doi.org/10.1007/978-981-96-1024-2_8},
  doi       = {10.1007/978-981-96-1024-2_8},
  booktitle = {Big Data},
  publisher = {Springer Nature Singapore},
  author    = {Yu, Hao and Gan, Aoran and Zhang, Kai and Tong, Shiwei and Liu, Qi and Liu, Zhaofeng},
  year      = {2025},
  pages     = {102-120}
}

@misc{zhao2024retrievalaugmentedgenerationaigeneratedcontent,
  title         = {Retrieval-Augmented Generation for AI-Generated Content: A Survey},
  author        = {Penghao Zhao and Hailin Zhang and Qinhan Yu and Zhengren Wang and Yunteng Geng and Fangcheng Fu and Ling Yang and Wentao Zhang and Jie Jiang and Bin Cui},
  year          = {2024},
  eprint        = {2402.19473},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2402.19473}
}

@InProceedings{Chan2025,
  author     = {Chan, Brian J. and Chen, Chao-Ting and Cheng, Jui-Hung and Huang, Hen-Hsen},
  booktitle  = {Companion Proceedings of the ACM on Web Conference 2025},
  title      = {Don’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks},
  doi        = {10.1145/3701716.3715490},
  pages      = {893–897},
  publisher  = {ACM},
  series     = {WWW ’25},
  url        = {http://dx.doi.org/10.1145/3701716.3715490},
  collection = {WWW ’25},
  month      = may,
  year       = {2025},
}

@InBook{Marvin2024,
  author    = {Marvin, Ggaliwango and Hellen, Nakayiza and Jjingo, Daudi and Nakatumba-Nabende, Joyce},
  booktitle = {Data Intelligence and Cognitive Informatics},
  date      = {2024},
  title     = {Prompt Engineering in Large Language Models},
  doi       = {10.1007/978-981-99-7962-2_30},
  isbn      = {9789819979622},
  pages     = {387--402},
  publisher = {Springer Nature Singapore},
  issn      = {2524-7573},
}

@Article{Hadi2023,
  author    = {Hadi, Muhammad Usman and tashi, qasem al and Qureshi, Rizwan and Shah, Abbas and muneer, amgad and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and Mirjalili, Seyedali},
  date      = {2023-07},
  title     = {A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage},
  doi       = {10.36227/techrxiv.23589741.v1},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@InBook{Ji2023,
  author    = {Ji, Ziwei and Yu, Tiezheng and Xu, Yan and Lee, Nayeon and Ishii, Etsuko and Fung, Pascale},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  date      = {2023},
  title     = {Towards Mitigating LLM Hallucination via Self Reflection},
  doi       = {10.18653/v1/2023.findings-emnlp.123},
  publisher = {Association for Computational Linguistics},
}

@Article{Raj2024,
  author    = {J, Mathav Raj and VM, Kushala and Warrier, Harikrishna and Gupta, Yogesh},
  date      = {2024},
  title     = {Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations},
  doi       = {10.48550/ARXIV.2404.10779},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Software Engineering (cs.SE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Balaguer2024,
  author       = {Balaguer, Angels and Benara, Vinamra and Cunha, Renato Luiz de Freitas and Hendry, Todd and Holstein, Daniel and Marsman, Jennifer and Mecklenburg, Nick and Malvar, Sara and Nunes, Leonardo O and Padilha, Rafael and others},
  date         = {2024},
  journaltitle = {arXiv preprint arXiv:2401.08406},
  title        = {RAG vs fine-tuning: pipelines, tradeoffs, and a case study on agriculture},
}

@Misc{Es2023,
  author    = {Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
  date      = {2023},
  title     = {Ragas: Automated Evaluation of Retrieval Augmented Generation},
  doi       = {10.48550/ARXIV.2309.15217},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@InProceedings{livebench,
  author    = {Colin White and Samuel Dooley and Manley Roberts and Arka Pal and Benjamin Feuer and Siddhartha Jain and Ravid Shwartz-Ziv and Neel Jain and Khalid Saifullah and Sreemanti Dey and Shubh-Agrawal and Sandeep Singh Sandha and Siddartha Venkat Naidu and Chinmay Hegde and Yann LeCun and Tom Goldstein and Willie Neiswanger and Micah Goldblum},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  title     = {LiveBench: A Challenging, Contamination-Free {LLM} Benchmark},
  year      = {2025},
}

@Misc{Zhang2019,
  author    = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  date      = {2019},
  title     = {BERTScore: Evaluating Text Generation with BERT},
  doi       = {10.48550/ARXIV.1904.09675},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@TechReport{hong2025context,
  author      = {Hong, Kelly and Troynikov, Anton and Huber, Jeff},
  institution = {Chroma},
  title       = {Context Rot: How Increasing Input Tokens Impacts LLM Performance},
  url         = {https://research.trychroma.com/context-rot},
  month       = {July},
  year        = {2025},
}

@Comment{jabref-meta: databaseType:biblatex;}
